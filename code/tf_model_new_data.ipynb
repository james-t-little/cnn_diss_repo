{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM4kpTIhtmvtoSVO0w6DNsd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/james-t-little/cnn_diss_repo/blob/main/code/tf_model_new_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Etn_9d6DBOd4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import PIL\n",
        "from google.colab import drive\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/dissertation_seg_data'\n",
        "\n",
        "input_path = os.path.join(path,\"input_patches/\")\n",
        "\n",
        "mask_path = os.path.join(path,\"mask_patches/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc6O2Ez9BoA0",
        "outputId": "aec54982-1d3e-4c36-c057-cfe028295ec8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image size that we are going to use\n",
        "IMG_SIZE = 512\n",
        "# Our images are RGB (3 channels)\n",
        "N_CHANNELS = 3\n",
        "# Scene Parsing has 150 classes + `not labeled`\n",
        "N_CLASSES = 3"
      ],
      "metadata": {
        "id": "2VkFjky4BpTZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_image(image_path: str) -> dict:\n",
        "    mask_path = tf.strings.regex_replace(image_path, \"image\", \"labels\")\n",
        "    mask_path = tf.strings.regex_replace(mask_path, \"input_patches\", \"mask_patches\")\n",
        "\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
        "\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels = 1)\n",
        "\n",
        "    mask = tf.where(mask == 255, np.dtype('uint8').type(0), mask)\n",
        "\n",
        "    return {'image' : image, 'segmentation_mask': mask}"
      ],
      "metadata": {
        "id": "fycO-k-NCpl0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.list_files(input_path + \"*.png\")\n",
        "\n",
        "train_size = 0.8 * train_dataset.cardinality().numpy()\n",
        "\n",
        "\n",
        "validation_size = 0.1 * train_dataset.cardinality().numpy()\n",
        "\n",
        "train_ds = train_dataset.take(train_size)\n",
        "\n",
        "extra = train_dataset.skip(train_size)  \n",
        "\n",
        "val_ds = extra.take(validation_size)\n",
        "\n",
        "test_ds = extra.skip(validation_size)\n",
        "\n",
        "#for filename in train_dataset.take(10):\n",
        "  #print(filename.numpy().decode('utf-8'))\n",
        "\n",
        "train_dataset = train_dataset.map(parse_image)\n",
        "\n",
        "train_ds = train_ds.map(parse_image)\n",
        "\n",
        "test_ds = test_ds.map(parse_image)\n",
        "\n",
        "val_ds = val_ds.map(parse_image)\n",
        "\n",
        "#for image in train_dataset.take(10):\n",
        "    #print(image['image'], image['segmentation_mask'],)"
      ],
      "metadata": {
        "id": "kewq4YgJC-dK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n",
        "    \n",
        "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "    \n",
        "    return input_image, input_mask\n",
        "\n",
        "def load_image_train(datapoint: dict) -> tuple:\n",
        "    \n",
        "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
        "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        input_image = tf.image.flip_left_right(input_image)\n",
        "        input_mask = tf.image.flip_left_right(input_mask)\n",
        "\n",
        "    input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "    return input_image, input_mask\n",
        "\n",
        "@tf.function\n",
        "def load_image_test(datapoint: dict) -> tuple:\n",
        "\n",
        "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
        "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    input_image, input_mask = normalize(input_image, input_mask)\n",
        "\n",
        "    return input_image, input_mask"
      ],
      "metadata": {
        "id": "YOzqp2O6EalI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "# for reference about the BUFFER_SIZE in shuffle:\n",
        "# https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "dataset = {\"train\": train_ds, \"test\": test_ds, \"val\": val_ds}\n",
        "\n",
        "dataset['train'] = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "dataset['train'] = dataset['train'].shuffle(buffer_size=BUFFER_SIZE, seed=47)\n",
        "dataset['train'] = dataset['train'].repeat()\n",
        "dataset['train'] = dataset['train'].batch(BATCH_SIZE)\n",
        "dataset['train'] = dataset['train'].prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "dataset['val'] = dataset['val'].map(load_image_test)\n",
        "dataset['val'] = dataset['val'].repeat()\n",
        "dataset['val'] = dataset['val'].batch(BATCH_SIZE)\n",
        "dataset['val'] = dataset['val'].prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "dataset['test'] = dataset['test'].map(load_image_test)\n",
        "dataset['test'] = dataset['test'].repeat()\n",
        "dataset['test'] = dataset['test'].batch(BATCH_SIZE)\n",
        "dataset['test'] = dataset['test'].prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "ulOm0-mgFIfI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, GaussianNoise\n",
        "from keras.layers import Conv2D, MaxPool2D, Input, concatenate, Conv2DTranspose\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras import regularizers, optimizers\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications import VGG16, InceptionV3\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "2lZJMT6_lg06"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unet(img_shape):\n",
        "    inputs = Input(shape=img_shape)\n",
        "\n",
        "    previous_block_activation = inputs\n",
        "\n",
        "    contraction = {}\n",
        "    for f in [32, 64, 128, 256]:\n",
        "        x = Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
        "        contraction[f'conv{f}'] = x\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "        previous_block_activation = x\n",
        "\n",
        "    c5 = Conv2D(160, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(previous_block_activation)\n",
        "    c5 = Dropout(0.2)(c5)\n",
        "    c5 = Conv2D(160, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "    previous_block_activation = c5\n",
        "\n",
        "    for f in reversed([32, 64, 128, 256]):\n",
        "        x = Conv2DTranspose(f, (2, 2), strides=(2, 2), padding='same')(previous_block_activation)\n",
        "        x = concatenate([x, contraction[f'conv{f}']])\n",
        "        x = Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Conv2D(f, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
        "        previous_block_activation = x\n",
        "\n",
        "    outputs = Conv2D(255, kernel_size=(1, 1), activation=\"softmax\")(previous_block_activation)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Free up RAM in case the model definition cells were run multiple times\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model = unet((IMG_SIZE, IMG_SIZE, N_CHANNELS))"
      ],
      "metadata": {
        "id": "kZAKQWAag7Q4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss = tf.keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "biohFttqi5Dk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 16\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "length_data = train_ds.cardinality().numpy()\n",
        "\n",
        "length_val = val_ds.cardinality().numpy()\n",
        "\n",
        "STEPS_PER_EPOCH = length_data // BATCH_SIZE\n",
        "VALIDATION_STEPS = length_val // BATCH_SIZE\n",
        "\n",
        "length_val"
      ],
      "metadata": {
        "id": "x-0QJN-bjIFi",
        "outputId": "ba96349b-c969-42b0-a5e6-6f7658968e27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4107"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_history = model.fit(dataset['train'], epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          validation_steps=VALIDATION_STEPS,\n",
        "                          validation_data=dataset['val'])"
      ],
      "metadata": {
        "id": "NNiuPXJdjJo-",
        "outputId": "7d54b000-123b-4ef4-a6ba-a123b5879dd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "8214/8214 [==============================] - 6619s 782ms/step - loss: 0.7540 - accuracy: 0.6826 - val_loss: 0.6509 - val_accuracy: 0.7168\n",
            "Epoch 2/16\n",
            "8214/8214 [==============================] - 2930s 357ms/step - loss: 0.5518 - accuracy: 0.7701 - val_loss: 0.5616 - val_accuracy: 0.7616\n",
            "Epoch 3/16\n",
            "8214/8214 [==============================] - 2462s 300ms/step - loss: 0.5075 - accuracy: 0.7908 - val_loss: 0.4994 - val_accuracy: 0.7916\n",
            "Epoch 4/16\n",
            "8214/8214 [==============================] - 2465s 300ms/step - loss: 0.4828 - accuracy: 0.8019 - val_loss: 0.4772 - val_accuracy: 0.8034\n",
            "Epoch 5/16\n",
            "8214/8214 [==============================] - 2464s 300ms/step - loss: 0.4645 - accuracy: 0.8099 - val_loss: 0.4667 - val_accuracy: 0.8069\n",
            "Epoch 6/16\n",
            "5089/8214 [=================>............] - ETA: 14:41 - loss: 0.4535 - accuracy: 0.8149"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/\"+\"lr0001_e16_bs4_zoom.h5\")"
      ],
      "metadata": {
        "id": "55MWnMSruciA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "loss_age = model_history.history['accuracy']\n",
        "val_loss_age = model_history.history['val_accuracy']"
      ],
      "metadata": {
        "id": "40PafUTdvQ7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_sample(display_list):\n",
        "    \n",
        "    plt.figure(figsize=(18, 18))\n",
        "\n",
        "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "    for i in range(len(display_list)):\n",
        "        plt.subplot(1, len(display_list), i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def create_mask(pred_mask: tf.Tensor) -> tf.Tensor:\n",
        "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "    pred_mask = tf.expand_dims(pred_mask, axis=-1)\n",
        "    return pred_mask\n",
        "\n",
        "for image, mask in dataset['test'].take(1):\n",
        "        pred_mask = model.predict(image)\n",
        "        display_sample([image[0], mask[0], create_mask(pred_mask[0])])"
      ],
      "metadata": {
        "id": "zfrxq8ZiCpd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Up3iA4d80boj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}